{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TransUnet feature Merge using different method and Pretrained on the new Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import io, os,sys,types\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '2'\n",
    "from torch import autograd\n",
    "from PIL import Image\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn.functional as F\n",
    "import torch.optim\n",
    "from torch.utils.data import Dataset\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "from IPython.display import clear_output\n",
    "\n",
    "import torch.utils.data\n",
    "\n",
    "import random\n",
    "import scipy.io\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from utils import self_attention,Fusion_block,CCA\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import average_precision_score\n",
    "\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "import optuna\n",
    "import shutil\n",
    "import pickle\n",
    "\n",
    "import torch.optim\n",
    "import torch.utils.data\n",
    "import pretrainedmodels\n",
    "from vit_pytorch import ViT\n",
    "from pytorch_pretrained_vit import ViT\n",
    "from networks import vit_seg_configs\n",
    "\n",
    "from networks.model import get_Model\n",
    "\n",
    "# from lion_pytorch import Lion\n",
    "import timm\n",
    "from tqdm import tqdm\n",
    "\n",
    "import nni\n",
    "from nni.experiment import Experiment\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param = {\n",
    "    'traindata_dir': '/path/to/train_img_dir',\n",
    "    'valdata_dir': '/path/to/val_img_dir',\n",
    "    'testdata_dir': '/path/to/test_img_dir',\n",
    "    'views_img_dir_list': ['view_1_img_dir_name', ..., 'view_v_img_dir_name'],\n",
    "    'modal_dir': '/path/to/model_parameters/',\n",
    "    'log_dir': '/path/to/training_log/',\n",
    "\n",
    "    'probs_loss_weight': 2,\n",
    "    'mass_mode': 'ebu',\n",
    "    'num_view': 5,\n",
    "    'num_classes': 2,\n",
    "    'ccc_loss_beta': 0.25,\n",
    "    'ccc_loss_so_weight': 2,\n",
    "    'contrastive_center_loss_lambda': 0.5,\n",
    "    'contrastive_center_loss_delta': 0.001,\n",
    "    'center_learning_rate': 0.00001,\n",
    "    'train_batch_size': 16, \n",
    "    'val_batch_size': 64,\n",
    "    'test_batch_size': 32,\n",
    "    'num_epochs': 200,\n",
    "    'patient_epoch': 20,\n",
    "    'l_lr': 0.00002,\n",
    "    'initial_lr': 0.00001,\n",
    "    'learning_rate': 0.00005,\n",
    "    'S_S_GAP': [0.1,0.1,0.1,0.1,0.1],\n",
    "    'w_momentum': 0.9,\n",
    "    'w_weight_decay': 0.00001,\n",
    "    'workers': 4,\n",
    "    'seed': 42,\n",
    "    'hidden_dim': 512,\n",
    "    'n_layers': 2,\n",
    "    'dropout': 0.5,\n",
    "    'loss-weight': torch.tensor(['class_0_weight',...,'class_c_weight'], dtype=torch.float32),\n",
    "    'seg-pretrained-weight':'/path/to/seg/model_pretrained_weight/U_Net.pth',\n",
    "    'seg_model_name':'U_Net_mid_output'#{transUnet:'transUnet',AttentionUnet:'AttentionUnet',transUnet_feature_fuse}\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DSF(nn.Module):\n",
    "    def __init__(self,mass_mode='ebu'):\n",
    "        super(DSF, self).__init__()\n",
    "        self.mass_mode = mass_mode\n",
    "        self.eps = 1e-8\n",
    "\n",
    "    def ebu_from_logits(self, logits):\n",
    "        \"\"\"\n",
    "        logits -> evidence(e) -> Dirichlet(alpha) -> EbU: (u, b)\n",
    "        input: logits (B, c)\n",
    "        output:\n",
    "          u: (B, 1)\n",
    "          b: (B, c)  且  sum(b, -1) + u = 1\n",
    "        \"\"\"\n",
    "        # evidence >= 0\n",
    "        e = F.softplus(logits)                    # (B, c)\n",
    "        alpha = e + 1.0                           # (B, c)\n",
    "        T = alpha.sum(dim=-1, keepdim=True)       # (B, 1)\n",
    "        c = logits.size(-1)\n",
    "        u = c / (T + self.eps)                         # (B, 1)\n",
    "        b = (alpha - 1.0) / (T + self.eps)             # (B, c)\n",
    "        # tensor clip\n",
    "        u = torch.clamp(u, self.eps, 1.0 - self.eps)\n",
    "        b = torch.clamp(b, self.eps, 1.0 - self.eps)\n",
    "        # tensor normalize\n",
    "        s = b.sum(dim=-1, keepdim=True) + u\n",
    "        b = b / (s + self.eps)                         # (B, c)\n",
    "        u = u / (s + self.eps)                         # (B, 1)\n",
    "        return b,u\n",
    "    \n",
    "    def to_mass(self, b, u):\n",
    "        \"\"\"\n",
    "        convert (u,b) to valid BBA: (m_single, m_theta)\n",
    "        - \"ebu\":         m_i = b_i, m_Theta = u\n",
    "        - \"discounted\":  first m_i~=(1-u)*b_i, m_Theta~=u, then normalize by Z=(1-u+u^2)\n",
    "        \"\"\"\n",
    "        if self.mass_mode == \"ebu\":\n",
    "            return b, u\n",
    "        if self.mass_mode == \"discounted\":\n",
    "            m_single = (1 - u) * b\n",
    "            m_theta = u\n",
    "            m_single = torch.clamp(m_single, self.eps, 1.0 - self.eps)\n",
    "            m_theta  = torch.clamp(m_theta,  self.eps, 1.0 - self.eps)\n",
    "            s = m_single.sum(dim=-1, keepdim=True) + m_theta\n",
    "            m_single = m_single / (s + self.eps)\n",
    "            m_theta = m_theta / (s + self.eps)\n",
    "            return m_single, m_theta\n",
    "        \n",
    "    def ds_fuse(self, b_list, u_list):\n",
    "        \"\"\"\n",
    "        multi-source DS fusion (all sources enter at once):\n",
    "          U = ∏ u_n\n",
    "          M_i = ∏ (u_n + b^{(n)}_i)\n",
    "          Q = Σ_i M_i - (C-1) * U\n",
    "          m*(Θ) = U / Q\n",
    "          m*({H_i}) = (M_i - U) / Q\n",
    "        return: m_star(single class quality, BxC), u_star(Bx1)\n",
    "        \"\"\"\n",
    "        # stack to (V,B,C) / (V,B,1)\n",
    "        m_stack = torch.stack(b_list, dim=0)        # (V,B,C) \n",
    "        u_stack = torch.stack(u_list, dim=0)        # (V,B,1)\n",
    "        # U = ∏ u_n\n",
    "        U = u_stack.prod(dim=0)                     # (B,1)\n",
    "\n",
    "        # M_i = ∏ (u_n + b^{(n)}_i)\n",
    "        term = u_stack + m_stack                    # (V,B,C)\n",
    "        M = term.prod(dim=0)                        # (B,C)\n",
    "\n",
    "        # Q = Σ_i M_i - (C-1) * U\n",
    "        B, C = M.shape\n",
    "        Q = M.sum(dim=-1, keepdim=True) - (C - 1) * U   # (B,1)\n",
    "        Q = torch.clamp(Q, min=self.eps)\n",
    " \n",
    "        m_theta = U / Q                          # (B,1)\n",
    "        m_star  = (M - U) / Q                    # (B,C)\n",
    "\n",
    "        # tensor clip and normalize\n",
    "        m_star  = torch.clamp(m_star, self.eps, 1.0 - self.eps)\n",
    "        m_theta = torch.clamp(m_theta, self.eps, 1.0 - self.eps)\n",
    "\n",
    "        Z = m_star.sum(dim=-1, keepdim=True) + m_theta\n",
    "        m_star  = m_star  / (Z + self.eps)\n",
    "        m_theta = m_theta / (Z + self.eps)\n",
    "        return m_star, m_theta\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        input:\n",
    "          logits: {view_name: feats(B,D_v)}\n",
    "        output:\n",
    "          probs: (B,C) with p_i = m*({H_i})\n",
    "          extras: diagnostic information\n",
    "        \"\"\"\n",
    "        b_list = []\n",
    "        u_list = []\n",
    "        for logits in x:\n",
    "            b, u = self.ebu_from_logits(logits)\n",
    "            b, u = self.to_mass(b, u)\n",
    "            b_list.append(b)\n",
    "            u_list.append(u)\n",
    "\n",
    "        m_fused, u_fused = self.ds_fuse(b_list, u_list)\n",
    "        probs = m_fused\n",
    "\n",
    "        return probs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EyeMVNet(nn.Module):\n",
    "    def __init__(self, convNeXt_raw, convNeXt_seg, seg_Net, mass_mode='ebu', num_view=5,num_class=2,hidden_dim=512, n_layers=2, dropout=0.5):\n",
    "        super(EyeMVNet, self).__init__()\n",
    "\n",
    "        self.num_view = num_view\n",
    "\n",
    "        self.seg_Net = seg_Net\n",
    "        convNeXt_raw.head.fc = nn.Linear(1024,1024)\n",
    "        convNeXt_seg.head.fc = nn.Linear(1024,1024)\n",
    "        self.convNeXt_raw_stem = convNeXt_raw.stem\n",
    "        self.convNeXt_seg_stem = convNeXt_seg.stem\n",
    "        \n",
    "        self.Unet_stage_0_feature_transform = nn.Conv2d(in_channels=128, out_channels=128, kernel_size=3, stride=2, padding=1)\n",
    "        self.Unet_stage_1_feature_transform = nn.Conv2d(in_channels=256, out_channels=256, kernel_size=3, stride=2, padding=1)\n",
    "        self.Unet_stage_2_feature_transform = nn.Conv2d(in_channels=512, out_channels=512, kernel_size=3, stride=2, padding=1)\n",
    "        self.Unet_stage_3_feature_transform = nn.Conv2d(in_channels=1024, out_channels=1024, kernel_size=3, stride=2, padding=1)\n",
    "\n",
    "        self.convNeXt_seg_stage0 = convNeXt_seg.stages[0]\n",
    "        self.convNeXt_seg_stage1 = convNeXt_seg.stages[1]\n",
    "        self.convNeXt_seg_stage2 = convNeXt_seg.stages[2]\n",
    "        self.convNeXt_seg_stage3 = convNeXt_seg.stages[3]\n",
    "\n",
    "        self.convNeXt_raw_stage0 = convNeXt_raw.stages[0]\n",
    "        self.convNeXt_raw_stage1 = convNeXt_raw.stages[1]\n",
    "        self.convNeXt_raw_stage2 = convNeXt_raw.stages[2]\n",
    "        self.convNeXt_raw_stage3 = convNeXt_raw.stages[3]\n",
    "\n",
    "        self.convNeXt_raw_head = convNeXt_raw.head\n",
    "        self.convNeXt_seg_head = convNeXt_seg.head\n",
    "\n",
    "        self.convNeXt_raw_linear1 = nn.Linear(1024,512)\n",
    "        self.convNeXt_raw_relu1 = nn.GELU()\n",
    "        self.convNeXt_raw_linear2 = nn.Linear(512,512)\n",
    "        self.convNeXt_raw_relu2 = nn.GELU()\n",
    "        # self.convNeXt_raw_linear3 = nn.Linear(512,2)\n",
    "\n",
    "        self.convNeXt_seg_linear1 = nn.Linear(1024,512)\n",
    "        self.convNeXt_seg_relu1 = nn.GELU()\n",
    "        self.convNeXt_seg_linear2 = nn.Linear(512,512)\n",
    "        self.convNeXt_seg_relu2 = nn.GELU()\n",
    "        # self.convNeXt_seg_linear3 = nn.Linear(512,2)\n",
    "\n",
    "        self.shared_linear1 = nn.Linear(1024,hidden_dim)\n",
    "        self.shared_relu1 = nn.GELU()\n",
    "\n",
    "        self.view_mlps_s = nn.ModuleList()\n",
    "        for i in range(num_view):\n",
    "            layers = []\n",
    "            for i in range(n_layers):\n",
    "                layers.append(nn.Linear(hidden_dim, hidden_dim))\n",
    "                layers.append(nn.GELU())\n",
    "                # layers.append(nn.Dropout(dropout))\n",
    "\n",
    "            self.view_mlps_s.append(nn.Sequential(*layers))\n",
    "\n",
    "        self.view_mlps_o = nn.ModuleList()\n",
    "        for i in range(num_view):\n",
    "            layers = []\n",
    "            for i in range(n_layers):\n",
    "                layers.append(nn.Linear(hidden_dim, hidden_dim))\n",
    "                layers.append(nn.GELU())\n",
    "                # layers.append(nn.Dropout(dropout))\n",
    "\n",
    "            self.view_mlps_o.append(nn.Sequential(*layers))\n",
    "\n",
    "        self.view_weights_mlps = nn.ModuleList()\n",
    "        for i in range(num_view):\n",
    "            layers = []\n",
    "            for i in range(n_layers):\n",
    "                layers.append(nn.Linear(hidden_dim, hidden_dim))\n",
    "                layers.append(nn.GELU())\n",
    "            layers.append(nn.Linear(hidden_dim,1))\n",
    "            layers.append(nn.Sigmoid())\n",
    "            self.view_weights_mlps.append(nn.Sequential(*layers))\n",
    "\n",
    "\n",
    "        self.views_last_linear_layers = nn.ModuleList()\n",
    "        for i in range(num_view):\n",
    "            layers = []\n",
    "            layers.append(nn.Linear(2*hidden_dim,hidden_dim))\n",
    "            layers.append(nn.GELU())\n",
    "            self.views_last_linear_layers.append(nn.Sequential(*layers))\n",
    "\n",
    "        self.views_cls_layers = nn.ModuleList()\n",
    "        for i in range(num_view):\n",
    "            self.views_cls_layers.append(nn.Linear(hidden_dim,num_class))\n",
    "\n",
    "        self.DSF = DSF(mass_mode=mass_mode)\n",
    "        \n",
    "        \n",
    "\n",
    "    def forward(self,multi_view_images):\n",
    "\n",
    "        shared_net_output = []\n",
    "        s_net_output = []\n",
    "        o_net_output = []\n",
    "        shared_net_output_weights = []\n",
    "        final_view_output = []\n",
    "        view_probs = []\n",
    "\n",
    "        for x in multi_view_images:\n",
    "\n",
    "            seg,unet_feature_map0,unet_feature_map1,unet_feature_map2,unet_feature_map3 = self.seg_Net(x)\n",
    "\n",
    "            seg_mask = seg[:,1,:,:]\n",
    "            seg_mask = seg_mask.unsqueeze(1)\n",
    "            seg_mask = seg_mask.repeat(1,3,1,1)\n",
    "            seg_raw = x*seg_mask\n",
    "\n",
    "            convNeXt_raw_output = self.convNeXt_raw_stem(x)\n",
    "            convNeXt_seg_output = self.convNeXt_seg_stem(seg_raw)\n",
    "\n",
    "            convNeXt_raw_stage0_feature = self.convNeXt_raw_stage0(convNeXt_raw_output)\n",
    "            convNeXt_seg_stage0_feature = self.convNeXt_seg_stage0(convNeXt_seg_output)\n",
    "            Unet_stage_0_feature = self.Unet_stage_0_feature_transform(unet_feature_map0)\n",
    "            convNeXt_seg_stage1_input = F.sigmoid(Unet_stage_0_feature) * convNeXt_seg_stage0_feature\n",
    "\n",
    "            convNeXt_raw_stage1_feature = self.convNeXt_raw_stage1(convNeXt_raw_stage0_feature)\n",
    "            convNeXt_seg_stage1_feature = self.convNeXt_seg_stage1(convNeXt_seg_stage1_input)\n",
    "            Unet_stage_1_feature = self.Unet_stage_1_feature_transform(unet_feature_map1)\n",
    "            convNeXt_seg_stage2_input = F.sigmoid(Unet_stage_1_feature) * convNeXt_seg_stage1_feature\n",
    "\n",
    "            convNeXt_raw_stage2_feature = self.convNeXt_raw_stage2(convNeXt_raw_stage1_feature)\n",
    "            convNeXt_seg_stage2_feature = self.convNeXt_seg_stage2(convNeXt_seg_stage2_input)\n",
    "            Unet_stage_2_feature = self.Unet_stage_2_feature_transform(unet_feature_map2)\n",
    "            convNeXt_seg_stage3_input = F.sigmoid(Unet_stage_2_feature) * convNeXt_seg_stage2_feature\n",
    "\n",
    "            convNeXt_raw_stage3_feature = self.convNeXt_raw_stage3(convNeXt_raw_stage2_feature)\n",
    "            convNeXt_seg_stage3_feature = self.convNeXt_seg_stage3(convNeXt_seg_stage3_input)\n",
    "            Unet_stage_3_feature = self.Unet_stage_3_feature_transform(unet_feature_map3)\n",
    "            convNeXt_seg_stage3_feature = F.sigmoid(Unet_stage_3_feature) * convNeXt_seg_stage3_feature\n",
    "            \n",
    "            convNeXt_raw_output = self.convNeXt_raw_head(convNeXt_raw_stage3_feature)\n",
    "            convNeXt_seg_output = self.convNeXt_seg_head(convNeXt_seg_stage3_feature)\n",
    "\n",
    "\n",
    "            convNeXt_raw_output = self.convNeXt_raw_linear1(convNeXt_raw_output)\n",
    "            convNeXt_raw_output = self.convNeXt_raw_relu1(convNeXt_raw_output)\n",
    "            convNeXt_raw_output = self.convNeXt_raw_linear2(convNeXt_raw_output)\n",
    "            convNeXt_raw_output = self.convNeXt_raw_relu2(convNeXt_raw_output)\n",
    "            # convNeXt_raw_logits = self.convNeXt_raw_linear3(convNeXt_raw_output)\n",
    "\n",
    "            convNeXt_seg_output = self.convNeXt_seg_linear1(convNeXt_seg_output)\n",
    "            convNeXt_seg_output = self.convNeXt_seg_relu1(convNeXt_seg_output)\n",
    "            convNeXt_seg_output = self.convNeXt_seg_linear2(convNeXt_seg_output)\n",
    "            convNeXt_seg_output = self.convNeXt_seg_relu2(convNeXt_seg_output)\n",
    "        # convNeXt_seg_logits = self.convNeXt_seg_linear3(convNeXt_seg_output)\n",
    "\n",
    "            output = torch.cat((convNeXt_raw_output, convNeXt_seg_output),1)\n",
    "            output = self.shared_linear1(output)\n",
    "            output = self.shared_relu1(output)\n",
    "            shared_net_output.append(output)\n",
    "\n",
    "        for index in range(self.num_view):\n",
    "            s_net_output.append(self.view_mlps_s[index](shared_net_output[index]))\n",
    "            o_net_output.append(self.view_mlps_o[index](shared_net_output[index]))\n",
    "            shared_net_output[index] = torch.cat((s_net_output[index],o_net_output[index]), 1)\n",
    "            # shared_net_output[index] = shared_net_output[index] * shared_net_output_weights[index]\n",
    "        # for i in range(len(s_net_output)):\n",
    "        #     # 每个view的shared特征与同view的proprietary特征串联\n",
    "        #     shared_net_output.append(torch.cat((s_net_output[i], torch.sigmoid(o_net_output[i])), 1))\n",
    "            \n",
    "        for index in range(self.num_view):\n",
    "            shared_net_output[index] = self.views_last_linear_layers[index](shared_net_output[index])\n",
    "            shared_net_output_weights.append(self.view_weights_mlps[index](shared_net_output[index]))\n",
    "            shared_net_output[index] = shared_net_output[index] * shared_net_output_weights[index]\n",
    "            final_view_output.append(self.views_cls_layers[index](shared_net_output[index]))\n",
    "        \n",
    "        \n",
    "        probs = self.DSF(final_view_output)\n",
    "            # view_probs.append(F.softmax(final_view_output[index], dim=1))\n",
    "\n",
    "        # probs = torch.stack(view_probs, dim=0).mean(dim=0)\n",
    "\n",
    "        \n",
    "        return probs, final_view_output, shared_net_output, s_net_output, o_net_output\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def specificity_score(y_true, y_pred, average='macro'):\n",
    "    \"\"\"\n",
    "    计算多分类问题的宏平均特异性\n",
    "    \n",
    "    参数:\n",
    "    y_true: 真实标签\n",
    "    y_pred: 预测标签\n",
    "    \n",
    "    返回:\n",
    "    macro_specificity: 宏平均特异性\n",
    "    \"\"\"\n",
    "    # 获取混淆矩阵\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    specificity_list = []\n",
    "    \n",
    "    for i in range(cm.shape[0]):\n",
    "        # 计算当前类别的真负例和假正例\n",
    "        tn = np.sum(cm) - np.sum(cm[i,:]) - np.sum(cm[:,i]) + cm[i,i]\n",
    "        fp = np.sum(cm[:,i]) - cm[i,i]\n",
    "        specificity = tn / (tn + fp)\n",
    "        specificity_list.append(specificity)\n",
    "        if average == 'macro':\n",
    "            return np.mean(specificity_list)\n",
    "        else:\n",
    "            raise ValueError(\"Only 'macro' are supported.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, loader, test_prob_dir, test_label_dir):\n",
    "\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "    \n",
    "        for name, parameters in model.named_parameters():\n",
    "            parameters.requires_grad = False\n",
    "\n",
    "        num_correct, num_samples = 0, len(loader.dataset)\n",
    "\n",
    "        test_batch_index = 0\n",
    "\n",
    "        for imgs, y in loader:\n",
    "\n",
    "            imgs_var= imgs.to(device)\n",
    "            scores, _, _, _, _ = model(imgs_var)\n",
    "            _, preds = scores.data.cpu().max(1)\n",
    "            if test_batch_index == 0:\n",
    "                y_test = y.cpu().detach().numpy()\n",
    "                y_probs = scores.cpu().detach().numpy()\n",
    "                y_pred = preds.numpy()\n",
    "            else:\n",
    "                y_test = np.append(y_test, y.cpu().detach().numpy())\n",
    "                y_probs = np.vstack((y_probs, scores.cpu().detach().numpy()))\n",
    "                y_pred = np.append(y_pred, preds.cpu().detach().numpy())\n",
    "\n",
    "            num_correct += (preds == y).sum()\n",
    "            test_batch_index += 1\n",
    "\n",
    "        acc = float(num_correct) / num_samples\n",
    "\n",
    "        print('Test accuracy: {:.2f}% ({}/{})'.format(\n",
    "            100.*acc,\n",
    "            num_correct,\n",
    "            num_samples,\n",
    "            ))\n",
    "\n",
    "        y_test_bin = label_binarize(y_test, classes=np.arange(y_probs.shape[1]))\n",
    "        AUC_score = roc_auc_score(y_test, y_probs, multi_class='ovr', average='macro')\n",
    "        AUPRC_score = average_precision_score(y_test_bin, y_probs, average='macro')\n",
    "        f1 = f1_score(y_test, y_pred, average='macro')\n",
    "        accuracy = accuracy_score(y_test, y_pred)\n",
    "        recall = recall_score(y_test, y_pred, average='macro')\n",
    "        precision = precision_score(y_test, y_pred, average='macro')\n",
    "        specificity = specificity_score(y_test, y_pred, average='macro')\n",
    "        \n",
    "\n",
    "        \n",
    "        print('AUC:')\n",
    "        print(AUC_score)\n",
    "        print('AUPRC:')\n",
    "        print(AUPRC_score)\n",
    "        print('recall:')\n",
    "        print(recall)\n",
    "        print('specificity:')\n",
    "        print(specificity)\n",
    "        print('F1:')\n",
    "        print(f1)\n",
    "        print('precision:')\n",
    "        print(precision)\n",
    "        print('Accuracy:')\n",
    "        print(accuracy)\n",
    "\n",
    "    return AUC_score, AUPRC_score, recall, specificity, f1, precision, accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###model name: transUnet:'transUnet', transUnet_feature_fuse:'transUnet_feature_fuse', AttentionUnet:'AttentionUnet', duckNet:'duckNet', Unet:'Unet'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiviewDataset(Dataset):\n",
    "    def __init__(self, views_img_dir_list, transform=None, target_transform=None):\n",
    "        self.image_dir_list = []\n",
    "        self.views_img_dir_list = views_img_dir_list\n",
    "        self.transform = transform\n",
    "        self.target_transform = target_transform\n",
    "        self.disease_class_list = os.listdir(self.views_img_dir_list[0])\n",
    "        for disease_class in self.disease_class_list:\n",
    "            self.class_image_name_list = os.listdir(os.path.join(self.views_img_dir_list[0], disease_class))\n",
    "            self.class_image_dir_list = [os.path.join(self.views_img_dir_list[0], disease_class,element) for element in self.class_image_name_list]\n",
    "            self.image_dir_list = self.image_dir_list + self.DRTiD_image_dir_list\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_dir_list)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image_list = []\n",
    "        first_view_image = Image.open(self.image_dir_list[idx]).convert('RGB')\n",
    "        image_list.append(first_view_image)\n",
    "        image_id = os.path.basename(self.image_dir_list[idx]).split('_')[0]\n",
    "        image_class = os.path.basename(os.path.dirname(self.image_dir_list[idx]))\n",
    "        for view_img_dir in self.views_img_dir_list[1:]:\n",
    "            next_view_image = Image.open(os.path.join(view_img_dir, image_class, image_id + '.png')).convert('RGB')\n",
    "            image_list.append(next_view_image)\n",
    "        label = int(image_class.split('_')[1])\n",
    "        label = torch.tensor(label, dtype=torch.long)\n",
    "        if self.transform:\n",
    "            for image in image_list:\n",
    "                image = self.transform(image)\n",
    "        if self.target_transform:\n",
    "            label = self.target_transform(label)\n",
    "        return image_list, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_views_testdata_dir_list = []\n",
    "for views_img_dir in range(param['views_img_dir_list']):\n",
    "    all_views_testdata_dir_list.append(os.path.join(param['testdata_dir'], views_img_dir))\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "MultiviewDataset(all_views_testdata_dir_list,\n",
    "                    transforms.Compose([\n",
    "    transforms.Resize((256,256)),\n",
    "    transforms.ToTensor()\n",
    "    \n",
    "])),\n",
    "batch_size=param['val_batch_size'], shuffle=False,\n",
    "num_workers=param['workers'], pin_memory=True)\n",
    "\n",
    "\n",
    "seg_model = get_Model(model_name=param['seg_model_name'])\n",
    "\n",
    "ConvNext_raw = timm.create_model(\"hf_hub:timm/convnext_base.fb_in22k_ft_in1k\", pretrained=True)\n",
    "ConvNext_seg = timm.create_model(\"hf_hub:timm/convnext_base.fb_in22k_ft_in1k\", pretrained=True)\n",
    "net = EyeMVNet(ConvNext_raw,ConvNext_seg,seg_model,mass_mode=param['mass_mode'] ,num_view=param['num_view'])\n",
    "net.to(device)\n",
    "for name, parameters in net.named_parameters():\n",
    "    parameters.requires_grad = False\n",
    "\n",
    "net_dir = os.path.join(model_dir, 'EyeMVNet_model_parameter.pkl')\n",
    "test_prob_dir = os.path.join(log_dir, 'EyeMVNet_test_prob.txt')\n",
    "test_label_dir = os.path.join(log_dir, 'EyeMVNet_test_label.txt')\n",
    "AUC_score, AUPRC_score, recall, specificity, f1, precision, accuracy = test(net, test_loader, test_prob_dir, test_label_dir)\n",
    "print('AUC_score:', AUC_score)\n",
    "print('AUPRC_score:', AUPRC_score)\n",
    "print('recall:', recall)\n",
    "print('specificity:', specificity)\n",
    "print('f1:', f1)\n",
    "print('precision:', precision)\n",
    "print('accuracy:', accuracy)\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
